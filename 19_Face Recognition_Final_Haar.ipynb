{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f298f6-0c81-40d8-8ea7-b751d09ca6ec",
   "metadata": {},
   "source": [
    "# 19. An End to End Face Recognition App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda1374-4498-4fbd-8727-9fcef44d447d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 1: Build  Image Data  for at least 2 people and Name Them with their Names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde959e4-82a0-44d2-9c05-9c85baea5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here \n",
    "# import numpy as np\n",
    "# import cv2 \n",
    "# import glob\n",
    "\n",
    "# imagesLeft = glob.glob('calib_images/stereoleft/*.png') \n",
    "# imagesRight = glob.glob('calib_images/stereoright/*.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eed3b2-d1cd-496c-b670-1e6aa1409ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical               \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571ce46-2581-4659-940e-1d216f0f022b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 2: Load The Data , Normalize them and resize them to (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0c5e9-58ae-4b7a-a7f1-7a904b0b6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir_1 = \"Abdulla-2/\" \n",
    "img_dir_2 = \"waleed/\" \n",
    "\n",
    "data_path_1 = os.path.join(img_dir_1,'*g') \n",
    "data_path_2 = os.path.join(img_dir_2,'*g') \n",
    "\n",
    "# files = glob.glob(data_path)\n",
    "images_abdo = glob.glob(data_path_1) \n",
    "images_toy = glob.glob(data_path_2)\n",
    "data = [] \n",
    "labels=[]\n",
    "\n",
    "for f1 in images_abdo: \n",
    "    img = cv2.imread(f1) \n",
    "    img = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
    "    img=img/255\n",
    "    # img=img.reshape(1,-1)\n",
    "    data.append(img)\n",
    "    labels.append(0)\n",
    "    \n",
    "for f1 in images_toy: \n",
    "    img = cv2.imread(f1) \n",
    "    img = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
    "    img=img/255\n",
    "    # img=img.reshape(1,-1)\n",
    "    data.append(img)\n",
    "    labels.append(1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8c749-9868-43cb-bb66-082a1d332347",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 3: Create Labels list  for example  [0,0,0,0,0,....,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97687a4f-6f6b-4398-84af-de41c8a7c29c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 4: Check if both classes are same count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb2b92-57c1-4830-af03-8571d651dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.count(0))\n",
    "print(labels.count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bbd11e-8b82-4afb-932f-7cf131c19aed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 5: Shuffle and Split Data into Trainig and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d0f3c-ed0d-409e-998e-015157e271df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.concatenate(data, axis=0)\n",
    "x, y = shuffle(data,labels, random_state=10)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y , random_state=0)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array([y_train])\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array([y_test])\n",
    "\n",
    "print(x_train.shape)\n",
    "y_train=y_train.transpose()\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "y_test=y_test.transpose()\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bd40e-8bbe-431b-8a1a-19d1fb538f17",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 6: Build Your Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bd5d2-b280-4157-9f8b-d7427fac14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fedd2-2eda-48de-8afa-8855f72e9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(2))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7d46d-6dad-4fa3-b2e6-5b261f70c136",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 7: Compile , Train , Evaluate your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d431d-c0b9-463e-8530-94cd5d609c60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#x_train, x_test, y_train, y_test\n",
    "#history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "# print(x_train)\n",
    "# print(x_test)\n",
    "\n",
    "# nb_classes = 2 # number of unique digits\n",
    "\n",
    "# Y_train = to_categorical(y_train, nb_classes)\n",
    "# Y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "# model.fit(x_train, Y_train)\n",
    "\n",
    "# print(Y_train)\n",
    "# model.fit(x_train,y_train)\n",
    "\n",
    "# y_pred = clf.predict(x_test)\n",
    "\n",
    "# acu = metrics.accuracy_score(y_test, y_pred)\n",
    "# acu\n",
    "\n",
    "# metrics.plot_confusion_matrix(clf, x_test, y_test, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25354e34-8861-40d0-bef9-e62e8db4e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_train, y_train)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['accuracy']\n",
    "loss_val = history.history['val_accuracy']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c71855",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x=model.predict(x_test) \n",
    "predicted_classes=np.argmax(predict_x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n",
    "correct_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[:1]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_test[correct], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
    "    \n",
    "plt.tight_layout()\n",
    "    \n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[:1]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_test[incorrect], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1712c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img =cv2.imread(\"valid/LiveFrame43.jpg\") \n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df817cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
    "# img=img/255\n",
    "\n",
    "# img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed10197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_img=[]\n",
    "# new_img.append(img)\n",
    "# x_New = np.array(new_img)\n",
    "# prediction = model.predict(x_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4252ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_classes=np.argmax(prediction,axis=1)\n",
    "# print(predicted_classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338c3ef-c5b1-4ab6-921d-6f04eeb47aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv2.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d3084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "img_dir_3 = \"valid-2/\" \n",
    "\n",
    "data_path_3 = os.path.join(img_dir_3,'*g') \n",
    "vaild = glob.glob(data_path_3) \n",
    "x_New=[]\n",
    "face_cascade = cv2.CascadeClassifier('C:/Users/DELL/anaconda3/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/DELL/anaconda3/Lib/site-packages/cv2/data/haarcascade_eye.xml')\n",
    "\n",
    "for f1 in vaild: \n",
    "    img = cv2.imread(f1)\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "        plt.imshow(img,'gray')\n",
    "        plt.show()\n",
    "    \n",
    "    img = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
    "    img=img/255\n",
    "    x_New.append(img)\n",
    "x_New = np.array(x_New)\n",
    "\n",
    "predict_x2=model.predict(x_New) \n",
    "predicted_classes2=np.argmax(predict_x2,axis=1)\n",
    "y_New=[]\n",
    "y_New[0:25]=[0]*25\n",
    "y_New[25:50]=[1]*25\n",
    "print(predicted_classes2)\n",
    "print(y_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b14f8a-2c21-4d2f-801d-4508e9726bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_indices2 = np.nonzero(predicted_classes2 == y_New)[0]\n",
    "incorrect_indices2 = np.nonzero(predicted_classes2 != y_New)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026af167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices2[:1]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_New[correct], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes2[correct], y_New[correct]))\n",
    "    \n",
    "plt.tight_layout()\n",
    "    \n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices2[:1]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_New[incorrect], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes2[incorrect], y_New[incorrect]))\n",
    "    \n",
    "plt.tight_layout()\n",
    "#y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c5621-de2f-4439-bbf6-c02d785fbf80",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 8: Inference Stage Let it be Live !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap =cv2.VideoCapture(0)   # 0 for Primary Camera , 1 for Secondary Camera , 2 ... etc\n",
    "                           #cv2.VideoCapture() args :  int Or \"Directory for Video\"\n",
    "face_cascade = cv2.CascadeClassifier('C:/Users/DELL/anaconda3/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/DELL/anaconda3/Lib/site-packages/cv2/data/haarcascade_eye.xml')\n",
    "\n",
    "while(1):\n",
    "    ret,frame = cap.read()  # reading each frame : ret is a boolean if reading is done correctly or not , frame\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #hsv=cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "   \n",
    "    frame = cv2.resize(frame, (224,224), interpolation = cv2.INTER_AREA)\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    frame=frame/255\n",
    "    x_New2=[]\n",
    "    x_New2.append(frame)\n",
    "    x_New2 = np.array(x_New2)\n",
    "\n",
    "    predict_x3=model.predict(x_New2) \n",
    "    predicted_classes3=np.argmax(predict_x3,axis=1)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        img = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "    \n",
    "    if predicted_classes3[0]==0:\n",
    "        print(\"Abdallah\")\n",
    "    else:\n",
    "        print(\"Waleed\")\n",
    "        \n",
    "    #y_New=[1,1 ,1 ,1, 1, 1, 1 ,1 ,1, 1, 0 ,0, 0, 0, 0, 0 ,0, 0 ,0, 0]\n",
    "    print(predicted_classes3)\n",
    "    \n",
    "    cv2.imshow('Live',frame)    \n",
    "    if cv2.waitKey(2000) & 0xFF == ord('q'):# 0xFF is a hexadecimal constant which is 11111111 in binary.\n",
    "        break                             # By using bitwise AND (&) with this constant,                                               \n",
    "cap.release()                             # it leaves only the last 8 bits of the original\n",
    "cv2.destroyAllWindows()                   #(in this case, whatever cv2.waitKey() is)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "cap = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier('C:/Users/DELL/anaconda3/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/DELL/anaconda3/Lib/site-packages/cv2/data/haarcascade_eye.xml')\n",
    "\n",
    "while(True):\n",
    "      \n",
    "    # Capture frames in the video\n",
    "    ret, frame = cap.read()\n",
    "    x_New2=[]\n",
    "    frame = cv2.resize(frame, (224,224), interpolation = cv2.INTER_AREA)\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    frame=frame/255\n",
    "    x_New2.append(frame)\n",
    "    x_New2 = np.array(x_New2)\n",
    "    predict_x3=model.predict(x_New2) \n",
    "    predicted_classes3=np.argmax(predict_x3,axis=1)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        img = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "            \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    if predicted_classes3[0]==0:\n",
    "        cv2.putText(frame, \n",
    "                'Abdulla',\n",
    "                (50, 50), \n",
    "                font, 1, \n",
    "                (0, 255, 255), \n",
    "                2, \n",
    "                cv2.LINE_4)\n",
    "       \n",
    "    else:\n",
    "        cv2.putText(frame, \n",
    "                'Waleed',\n",
    "                (50, 50), \n",
    "                font, 1, \n",
    "                (0, 255, 255), \n",
    "                2, \n",
    "                cv2.LINE_4)\n",
    "        \n",
    "        \n",
    "  \n",
    "    # describe the type of font\n",
    "    # to be used.\n",
    "  \n",
    "    # Use putText() method for\n",
    "    # inserting text on video\n",
    "    \n",
    "  \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('video', frame)\n",
    "  \n",
    "    # creating 'q' as the quit \n",
    "    # button for the video\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "  \n",
    "cap.release()\n",
    "# close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f062e-7ec1-417b-890c-34dce00220cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 9: Build Tkinter App for Complete End to End Face Recognition App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dc657-4bb6-41ff-a76c-bd28c9e40c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
